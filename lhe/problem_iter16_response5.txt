```python
import torch

def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:
    n = distance_matrix.size(0)

    # Normalize demands
    max_demand = 1.0 - demands[0]
    demands_norm = demands / torch.sum(demands)

    # Calculate the mean distance
    mean_distance = distance_matrix.mean()

    # Initialize attentional bias matrix
    attentional_bias = torch.zeros_like(distance_matrix)

    # Calculate the attentional bias matrix using efficient vectorized implementation
    attentional_bias = distance_matrix / mean_distance * max_demand / demands_norm

    # Set zero values for NaN and Inf
    attentional_bias[torch.isnan(attentional_bias)] = 0
    attentional_bias[torch.isinf(attentional_bias)] = 0

    return attentional_bias
```
The above code is an improved version of the `heuristics` function, `heuristics_v1`. The improvements include the use of efficient vectorized implementations, normalizing the demands, and calculating the mean distance. It handles NaN and Inf values by setting them to zero in the attentional bias matrix.
