```python
import torch
import torch.nn.functional as F

def heuristics_v2(distance_matrix: torch.Tensor, demands: torch.Tensor) -> torch.Tensor:

    # Normalize demand based on the maximum demand
    max_demand = demands.max().item()
    normalized_demands = demands / max_demand
    
    # Calculate the attentional bias
    attentional_bias = distance_matrix / normalized_demands.unsqueeze(1)
    
    # Set negative values and nan/inf values to -inf
    attentional_bias.masked_fill_(attentional_bias <= 0, float('-inf'))
    
    # Set self-edges values to 0
    attentional_bias.fill_diagonal_(0)
    
    return attentional_bias
```
The code above improves the previous version by using masked_fill_ and fill_diagonal_ functions for more efficient operations on the attentional bias.
